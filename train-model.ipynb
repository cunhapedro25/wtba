{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install ultralytics"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download multiple datasets and merging",
   "id": "713de19f4b3783be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T09:05:25.762614Z",
     "start_time": "2025-06-06T08:31:27.220089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from roboflow import Roboflow\n",
    "\n",
    "class DatasetCombiner:\n",
    "    def __init__(self, api_key, output_dir=\"combined_dataset\"):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv('ROBOFLOW_API_KEY')\n",
    "            if api_key is None:\n",
    "                raise ValueError(\"API key must be provided either as parameter or ROBOFLOW_API_KEY environment variable\")\n",
    "\n",
    "\n",
    "        self.rf = Roboflow(api_key=api_key)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.target_classes = ['hog', 'boar', 'wild boar', 'pig', 'pigeon', 'rabbit', 'bunny', 'deer', 'squirrel']\n",
    "        self.class_mapping = {\n",
    "            # Common variations for wild boar/hog\n",
    "            'hog': 'hog',\n",
    "            'boar': 'hog',\n",
    "            'wild boar': 'hog',\n",
    "            'wild_boar': 'hog',\n",
    "            'pig': 'hog',\n",
    "            'swine': 'hog',\n",
    "\n",
    "            # Pigeon variations\n",
    "            'pigeon': 'pigeon',\n",
    "            'dove': 'pigeon',\n",
    "            'bird': 'pigeon',  # Only if context suggests pigeon\n",
    "\n",
    "            # Rabbit variations\n",
    "            'rabbit': 'rabbit',\n",
    "            'bunny': 'rabbit',\n",
    "            'hare': 'rabbit',\n",
    "\n",
    "            # Deer variations\n",
    "            'deer': 'deer',\n",
    "            'roe deer': 'deer',\n",
    "            'red deer': 'deer',\n",
    "            'stag': 'deer',\n",
    "            'doe': 'deer',\n",
    "            'buck': 'deer',\n",
    "\n",
    "            # Squirrel variations\n",
    "            'squirrel': 'squirrel',\n",
    "            'red squirrel': 'squirrel',\n",
    "            'grey squirrel': 'squirrel',\n",
    "            'gray squirrel': 'squirrel'\n",
    "        }\n",
    "        self.final_classes = ['hog', 'pigeon', 'rabbit', 'deer', 'squirrel']\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"Download all datasets from Roboflow\"\"\"\n",
    "        datasets = []\n",
    "\n",
    "        # Dataset configurations\n",
    "        dataset_configs = [\n",
    "            (\"aflevering1\", \"my-first-project-xub7r\", 1),\n",
    "            (\"my-game-pics\", \"my-game-pics\", 7),\n",
    "            (\"animaldetection-rvmi9\", \"animal_detection-7wsk6\", 5),\n",
    "            (\"uncocos\", \"pigeon-v8l3q\", 6),\n",
    "            (\"trail-camera-training\", \"trailcam-detection\", 2)\n",
    "        ]\n",
    "\n",
    "        print(\"Downloading datasets...\")\n",
    "        for i, (workspace, project_name, version_num) in enumerate(dataset_configs, 1):\n",
    "            try:\n",
    "                print(f\"Downloading dataset {i}/5: {project_name}\")\n",
    "                project = self.rf.workspace(workspace).project(project_name)\n",
    "                version = project.version(version_num)\n",
    "                dataset = version.download(\"yolov11\", location=f\"temp_dataset_{i}\")\n",
    "                datasets.append(f\"temp_dataset_{i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading dataset {i}: {e}\")\n",
    "\n",
    "        return datasets\n",
    "\n",
    "    def load_yaml_config(self, dataset_path):\n",
    "        \"\"\"Load YAML configuration from dataset\"\"\"\n",
    "        yaml_path = Path(dataset_path) / \"data.yaml\"\n",
    "        if yaml_path.exists():\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        return None\n",
    "\n",
    "    def should_keep_class(self, class_name):\n",
    "        \"\"\"Check if a class should be kept based on target animals\"\"\"\n",
    "        class_lower = class_name.lower().strip()\n",
    "\n",
    "        # Direct matches\n",
    "        if class_lower in [c.lower() for c in self.target_classes]:\n",
    "            return True\n",
    "\n",
    "        # Partial matches for compound names\n",
    "        for target in self.target_classes:\n",
    "            if target.lower() in class_lower or class_lower in target.lower():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def map_class_name(self, original_name):\n",
    "        \"\"\"Map original class name to standardized name\"\"\"\n",
    "        original_lower = original_name.lower().strip()\n",
    "\n",
    "        # Direct mapping\n",
    "        if original_lower in self.class_mapping:\n",
    "            return self.class_mapping[original_lower]\n",
    "\n",
    "        # Partial matching\n",
    "        for key, value in self.class_mapping.items():\n",
    "            if key in original_lower or original_lower in key:\n",
    "                return value\n",
    "\n",
    "        # Default mapping based on keywords\n",
    "        if any(keyword in original_lower for keyword in ['boar', 'hog', 'pig', 'swine']):\n",
    "            return 'hog'\n",
    "        elif any(keyword in original_lower for keyword in ['pigeon', 'dove']):\n",
    "            return 'pigeon'\n",
    "        elif any(keyword in original_lower for keyword in ['rabbit', 'bunny', 'hare']):\n",
    "            return 'rabbit'\n",
    "        elif any(keyword in original_lower for keyword in ['deer', 'stag', 'doe', 'buck']):\n",
    "            return 'deer'\n",
    "        elif any(keyword in original_lower for keyword in ['squirrel']):\n",
    "            return 'squirrel'\n",
    "\n",
    "        return original_name.lower().replace(' ', '_')\n",
    "\n",
    "    def process_annotations(self, annotation_file, class_names, new_class_mapping):\n",
    "        \"\"\"Process YOLO annotation file and filter/remap classes\"\"\"\n",
    "        if not os.path.exists(annotation_file):\n",
    "            return []\n",
    "\n",
    "        new_annotations = []\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id < len(class_names):\n",
    "                        original_class = class_names[class_id]\n",
    "                        if self.should_keep_class(original_class):\n",
    "                            mapped_class = self.map_class_name(original_class)\n",
    "                            if mapped_class in new_class_mapping:\n",
    "                                new_class_id = new_class_mapping[mapped_class]\n",
    "                                new_line = f\"{new_class_id} {' '.join(parts[1:])}\"\n",
    "                                new_annotations.append(new_line)\n",
    "\n",
    "        return new_annotations\n",
    "\n",
    "    def combine_datasets(self, dataset_paths):\n",
    "        \"\"\"Combine multiple datasets into one unified dataset\"\"\"\n",
    "        print(\"Combining datasets...\")\n",
    "\n",
    "        # Create output directory structure\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            (self.output_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create new class mapping\n",
    "        new_class_mapping = {class_name: i for i, class_name in enumerate(self.final_classes)}\n",
    "\n",
    "        image_counter = 0\n",
    "        stats = defaultdict(int)\n",
    "\n",
    "        for dataset_path in dataset_paths:\n",
    "            if not os.path.exists(dataset_path):\n",
    "                print(f\"Dataset path {dataset_path} not found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing dataset: {dataset_path}\")\n",
    "\n",
    "            # Load dataset configuration\n",
    "            config = self.load_yaml_config(dataset_path)\n",
    "            if not config or 'names' not in config:\n",
    "                print(f\"No valid config found for {dataset_path}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            original_classes = config['names']\n",
    "            if isinstance(original_classes, dict):\n",
    "                original_classes = list(original_classes.values())\n",
    "\n",
    "            # Process each split\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                split_path = Path(dataset_path) / split\n",
    "                if not split_path.exists():\n",
    "                    continue\n",
    "\n",
    "                images_path = split_path / 'images'\n",
    "                labels_path = split_path / 'labels'\n",
    "\n",
    "                if not images_path.exists():\n",
    "                    continue\n",
    "\n",
    "                # Process each image and its annotation\n",
    "                for img_file in images_path.glob('*'):\n",
    "                    if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                        # Find corresponding label file\n",
    "                        label_file = labels_path / f\"{img_file.stem}.txt\"\n",
    "\n",
    "                        # Process annotations\n",
    "                        new_annotations = self.process_annotations(\n",
    "                            str(label_file), original_classes, new_class_mapping\n",
    "                        )\n",
    "\n",
    "                        # Only keep images that have valid annotations\n",
    "                        if new_annotations:\n",
    "                            # Copy image with new name\n",
    "                            new_img_name = f\"img_{image_counter:06d}{img_file.suffix}\"\n",
    "                            new_img_path = self.output_dir / split / 'images' / new_img_name\n",
    "                            shutil.copy2(img_file, new_img_path)\n",
    "\n",
    "                            # Write new annotation file\n",
    "                            new_label_path = self.output_dir / split / 'labels' / f\"img_{image_counter:06d}.txt\"\n",
    "                            with open(new_label_path, 'w') as f:\n",
    "                                f.write('\\n'.join(new_annotations) + '\\n')\n",
    "\n",
    "                            image_counter += 1\n",
    "                            stats[split] += 1\n",
    "\n",
    "                            # Count classes\n",
    "                            for ann in new_annotations:\n",
    "                                class_id = int(ann.split()[0])\n",
    "                                class_name = self.final_classes[class_id]\n",
    "                                stats[f\"{split}_{class_name}\"] += 1\n",
    "\n",
    "        # Create new data.yaml\n",
    "        new_config = {\n",
    "            'path': str(self.output_dir.absolute()),\n",
    "            'train': 'train/images',\n",
    "            'val': 'valid/images',\n",
    "            'test': 'test/images',\n",
    "            'nc': len(self.final_classes),\n",
    "            'names': {i: name for i, name in enumerate(self.final_classes)}\n",
    "        }\n",
    "\n",
    "        # Write test split to val if test doesn't exist\n",
    "        if stats.get('test', 0) == 0 and stats.get('valid', 0) == 0:\n",
    "            print(\"No validation or test set found, creating validation split from train...\")\n",
    "            self.create_validation_split()\n",
    "            new_config['val'] = 'valid/images'\n",
    "\n",
    "        with open(self.output_dir / 'data.yaml', 'w') as f:\n",
    "            yaml.dump(new_config, f, default_flow_style=False)\n",
    "\n",
    "        # Print statistics\n",
    "        self.print_statistics(stats)\n",
    "\n",
    "        return str(self.output_dir)\n",
    "\n",
    "    def create_validation_split(self, val_ratio=0.2):\n",
    "        \"\"\"Create validation split from training data\"\"\"\n",
    "        import random\n",
    "\n",
    "        train_images = list((self.output_dir / 'train' / 'images').glob('*'))\n",
    "        random.shuffle(train_images)\n",
    "\n",
    "        val_count = int(len(train_images) * val_ratio)\n",
    "        val_images = train_images[:val_count]\n",
    "\n",
    "        for img_path in val_images:\n",
    "            # Move image\n",
    "            val_img_path = self.output_dir / 'valid' / 'images' / img_path.name\n",
    "            shutil.move(str(img_path), str(val_img_path))\n",
    "\n",
    "            # Move corresponding label\n",
    "            label_path = self.output_dir / 'train' / 'labels' / f\"{img_path.stem}.txt\"\n",
    "            if label_path.exists():\n",
    "                val_label_path = self.output_dir / 'valid' / 'labels' / f\"{img_path.stem}.txt\"\n",
    "                shutil.move(str(label_path), str(val_label_path))\n",
    "\n",
    "    def print_statistics(self, stats):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        total_images = sum(stats[split] for split in ['train', 'valid', 'test'] if split in stats)\n",
    "        print(f\"Total images: {total_images}\")\n",
    "\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            if split in stats:\n",
    "                print(f\"{split.capitalize()} images: {stats[split]}\")\n",
    "\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for class_name in self.final_classes:\n",
    "            total_instances = sum(stats.get(f\"{split}_{class_name}\", 0)\n",
    "                                  for split in ['train', 'valid', 'test'])\n",
    "            print(f\"  {class_name}: {total_instances} instances\")\n",
    "\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                split_count = stats.get(f\"{split}_{class_name}\", 0)\n",
    "                if split_count > 0:\n",
    "                    print(f\"    {split}: {split_count}\")\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def cleanup_temp_datasets(self, dataset_paths):\n",
    "        \"\"\"Remove temporary downloaded datasets\"\"\"\n",
    "        print(\"Cleaning up temporary files...\")\n",
    "        for dataset_path in dataset_paths:\n",
    "            if os.path.exists(dataset_path):\n",
    "                shutil.rmtree(dataset_path)\n",
    "        print(\"Cleanup completed!\")\n",
    "\n",
    "    def run(self, cleanup=True):\n",
    "        \"\"\"Main execution function\"\"\"\n",
    "        try:\n",
    "            # Download datasets\n",
    "            dataset_paths = self.download_datasets()\n",
    "\n",
    "            if not dataset_paths:\n",
    "                print(\"No datasets were downloaded successfully!\")\n",
    "                return None\n",
    "\n",
    "            # Combine datasets\n",
    "            combined_path = self.combine_datasets(dataset_paths)\n",
    "\n",
    "            # Cleanup temporary files\n",
    "            if cleanup:\n",
    "                self.cleanup_temp_datasets(dataset_paths)\n",
    "\n",
    "            print(f\"\\nDataset combination completed!\")\n",
    "            print(f\"Combined dataset saved to: {combined_path}\")\n",
    "            print(f\"Use the data.yaml file for YOLO training\")\n",
    "\n",
    "            return combined_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dataset combination: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "# Initialize the combiner\n",
    "combiner = DatasetCombiner(\n",
    "    api_key=\"D17qu78OWJlZOB5KchPA\",\n",
    "    output_dir=\"wildlife_dataset\"\n",
    ")\n",
    "\n",
    "# Run the combination process\n",
    "result_path = combiner.run(cleanup=True)\n",
    "\n",
    "if result_path:\n",
    "    print(f\"\\nüéâ Success! Your dataset is ready at: {result_path}\")\n",
    "    print(\"\\nTo train with YOLO:\")\n",
    "    print(f\"yolo train data={result_path}/data.yaml model=yolo11n.pt epochs=100 imgsz=640\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset combination failed!\")"
   ],
   "id": "a6a5c54d29021735",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedro/Documents/school/ano3/s2/aoop/projects/wtba/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets...\n",
      "Downloading dataset 1/5: my-first-project-xub7r\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_1 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550910/550910 [03:33<00:00, 2582.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to temp_dataset_1 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23060/23060 [00:02<00:00, 8503.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 2/5: my-game-pics\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_2 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 511120/511120 [03:40<00:00, 2322.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to temp_dataset_2 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24270/24270 [00:05<00:00, 4249.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 3/5: animal_detection-7wsk6\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_3 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2128460/2128460 [16:30<00:00, 2148.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to temp_dataset_3 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5206/5206 [00:03<00:00, 1378.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 4/5: pigeon-v8l3q\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_4 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182867/182867 [01:16<00:00, 2399.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to temp_dataset_4 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5480/5480 [00:00<00:00, 7985.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 5/5: trailcam-detection\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_5 to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1157698/1157698 [07:52<00:00, 2451.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to temp_dataset_5 in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31382/31382 [00:03<00:00, 9015.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "Processing dataset: temp_dataset_1\n",
      "Processing dataset: temp_dataset_2\n",
      "Processing dataset: temp_dataset_3\n",
      "Processing dataset: temp_dataset_4\n",
      "Processing dataset: temp_dataset_5\n",
      "\n",
      "==================================================\n",
      "DATASET STATISTICS\n",
      "==================================================\n",
      "Total images: 33862\n",
      "Train images: 30283\n",
      "Valid images: 2018\n",
      "Test images: 1561\n",
      "\n",
      "Class distribution:\n",
      "  hog: 21641 instances\n",
      "    train: 19665\n",
      "    valid: 1408\n",
      "    test: 568\n",
      "  pigeon: 4821 instances\n",
      "    train: 3535\n",
      "    valid: 542\n",
      "    test: 744\n",
      "  rabbit: 955 instances\n",
      "    train: 885\n",
      "    valid: 45\n",
      "    test: 25\n",
      "  deer: 25691 instances\n",
      "    train: 23317\n",
      "    valid: 1405\n",
      "    test: 969\n",
      "  squirrel: 0 instances\n",
      "==================================================\n",
      "Cleaning up temporary files...\n",
      "Cleanup completed!\n",
      "\n",
      "Dataset combination completed!\n",
      "Combined dataset saved to: wildlife_dataset\n",
      "Use the data.yaml file for YOLO training\n",
      "\n",
      "üéâ Success! Your dataset is ready at: wildlife_dataset\n",
      "\n",
      "To train with YOLO:\n",
      "yolo train data=wildlife_dataset/data.yaml model=yolo11n.pt epochs=100 imgsz=640\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "aa831c206720e940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!yolo detect train data=/teamspace/studios/this_studio/wildlife_dataset/data.yaml model=yolo11n.pt epochs=100 imgsz=640",
   "id": "28383a1299cca57d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
