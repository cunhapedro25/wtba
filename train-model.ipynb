{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download multiple datasets and merging",
   "id": "713de19f4b3783be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:51:25.031532Z",
     "start_time": "2025-06-07T20:51:08.514558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from roboflow import Roboflow\n",
    "\n",
    "class DatasetCombiner:\n",
    "    def __init__(self, api_key, output_dir=\"combined_dataset\"):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv('ROBOFLOW_API_KEY')\n",
    "            if api_key is None:\n",
    "                raise ValueError(\"API key must be provided either as parameter or ROBOFLOW_API_KEY environment variable\")\n",
    "\n",
    "\n",
    "        self.rf = Roboflow(api_key=api_key)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.target_classes = ['hog', 'boar', 'wild boar', 'pig', 'pigeon', 'rabbit', 'bunny', 'deer']\n",
    "        self.class_mapping = {\n",
    "            # Common variations for wild boar/hog\n",
    "            'hog': 'hog',\n",
    "            'boar': 'hog',\n",
    "            'wild boar': 'hog',\n",
    "            'wild_boar': 'hog',\n",
    "            'pig': 'hog',\n",
    "            'swine': 'hog',\n",
    "\n",
    "            # Pigeon variations\n",
    "            'pigeon': 'pigeon',\n",
    "            'dove': 'pigeon',\n",
    "            'bird': 'pigeon',  # Only if context suggests pigeon\n",
    "\n",
    "            # Rabbit variations\n",
    "            'rabbit': 'rabbit',\n",
    "            'bunny': 'rabbit',\n",
    "            'hare': 'rabbit',\n",
    "\n",
    "            # Deer variations\n",
    "            'deer': 'deer',\n",
    "            'roe deer': 'deer',\n",
    "            'red deer': 'deer',\n",
    "            'stag': 'deer',\n",
    "            'doe': 'deer',\n",
    "            'buck': 'deer',\n",
    "        }\n",
    "        self.final_classes = ['hog', 'pigeon', 'rabbit', 'deer']\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"Download all datasets from Roboflow\"\"\"\n",
    "        datasets = []\n",
    "\n",
    "        # Dataset configurations\n",
    "        dataset_configs = [\n",
    "            (\"aflevering1\", \"my-first-project-xub7r\", 1),\n",
    "            (\"my-game-pics\", \"my-game-pics\", 7),\n",
    "            (\"animaldetection-rvmi9\", \"animal_detection-7wsk6\", 5),\n",
    "            (\"uncocos\", \"pigeon-v8l3q\", 6),\n",
    "            (\"trail-camera-training\", \"trailcam-detection\", 2)\n",
    "        ]\n",
    "\n",
    "        print(\"Downloading datasets...\")\n",
    "        for i, (workspace, project_name, version_num) in enumerate(dataset_configs, 1):\n",
    "            try:\n",
    "                print(f\"Downloading dataset {i}/5: {project_name}\")\n",
    "                project = self.rf.workspace(workspace).project(project_name)\n",
    "                version = project.version(version_num)\n",
    "                dataset = version.download(\"yolov11\", location=f\"temp_dataset_{i}\")\n",
    "                datasets.append(f\"temp_dataset_{i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading dataset {i}: {e}\")\n",
    "\n",
    "        return datasets\n",
    "\n",
    "    def load_yaml_config(self, dataset_path):\n",
    "        \"\"\"Load YAML configuration from dataset\"\"\"\n",
    "        yaml_path = Path(dataset_path) / \"data.yaml\"\n",
    "        if yaml_path.exists():\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        return None\n",
    "\n",
    "    def should_keep_class(self, class_name):\n",
    "        \"\"\"Check if a class should be kept based on target animals\"\"\"\n",
    "        class_lower = class_name.lower().strip()\n",
    "\n",
    "        # Direct matches\n",
    "        if class_lower in [c.lower() for c in self.target_classes]:\n",
    "            return True\n",
    "\n",
    "        # Partial matches for compound names\n",
    "        for target in self.target_classes:\n",
    "            if target.lower() in class_lower or class_lower in target.lower():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def map_class_name(self, original_name):\n",
    "        \"\"\"Map original class name to standardized name\"\"\"\n",
    "        original_lower = original_name.lower().strip()\n",
    "\n",
    "        # Direct mapping\n",
    "        if original_lower in self.class_mapping:\n",
    "            return self.class_mapping[original_lower]\n",
    "\n",
    "        # Partial matching\n",
    "        for key, value in self.class_mapping.items():\n",
    "            if key in original_lower or original_lower in key:\n",
    "                return value\n",
    "\n",
    "        # Default mapping based on keywords\n",
    "        if any(keyword in original_lower for keyword in ['boar', 'hog', 'pig', 'swine']):\n",
    "            return 'hog'\n",
    "        elif any(keyword in original_lower for keyword in ['pigeon', 'dove']):\n",
    "            return 'pigeon'\n",
    "        elif any(keyword in original_lower for keyword in ['rabbit', 'bunny', 'hare']):\n",
    "            return 'rabbit'\n",
    "        elif any(keyword in original_lower for keyword in ['deer', 'stag', 'doe', 'buck']):\n",
    "            return 'deer'\n",
    "\n",
    "        return original_name.lower().replace(' ', '_')\n",
    "\n",
    "    def process_annotations(self, annotation_file, class_names, new_class_mapping):\n",
    "        \"\"\"Process YOLO annotation file and filter/remap classes\"\"\"\n",
    "        if not os.path.exists(annotation_file):\n",
    "            return []\n",
    "\n",
    "        new_annotations = []\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id < len(class_names):\n",
    "                        original_class = class_names[class_id]\n",
    "                        if self.should_keep_class(original_class):\n",
    "                            mapped_class = self.map_class_name(original_class)\n",
    "                            if mapped_class in new_class_mapping:\n",
    "                                new_class_id = new_class_mapping[mapped_class]\n",
    "                                new_line = f\"{new_class_id} {' '.join(parts[1:])}\"\n",
    "                                new_annotations.append(new_line)\n",
    "\n",
    "        return new_annotations\n",
    "\n",
    "    def combine_datasets(self, dataset_paths):\n",
    "        \"\"\"Combine multiple datasets into one unified dataset\"\"\"\n",
    "        print(\"Combining datasets...\")\n",
    "\n",
    "        # Create output directory structure\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            (self.output_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create new class mapping\n",
    "        new_class_mapping = {class_name: i for i, class_name in enumerate(self.final_classes)}\n",
    "\n",
    "        image_counter = 0\n",
    "        stats = defaultdict(int)\n",
    "\n",
    "        for dataset_path in dataset_paths:\n",
    "            if not os.path.exists(dataset_path):\n",
    "                print(f\"Dataset path {dataset_path} not found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing dataset: {dataset_path}\")\n",
    "\n",
    "            # Load dataset configuration\n",
    "            config = self.load_yaml_config(dataset_path)\n",
    "            if not config or 'names' not in config:\n",
    "                print(f\"No valid config found for {dataset_path}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            original_classes = config['names']\n",
    "            if isinstance(original_classes, dict):\n",
    "                original_classes = list(original_classes.values())\n",
    "\n",
    "            # Process each split\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                split_path = Path(dataset_path) / split\n",
    "                if not split_path.exists():\n",
    "                    continue\n",
    "\n",
    "                images_path = split_path / 'images'\n",
    "                labels_path = split_path / 'labels'\n",
    "\n",
    "                if not images_path.exists():\n",
    "                    continue\n",
    "\n",
    "                # Process each image and its annotation\n",
    "                for img_file in images_path.glob('*'):\n",
    "                    if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                        # Find corresponding label file\n",
    "                        label_file = labels_path / f\"{img_file.stem}.txt\"\n",
    "\n",
    "                        # Process annotations\n",
    "                        new_annotations = self.process_annotations(\n",
    "                            str(label_file), original_classes, new_class_mapping\n",
    "                        )\n",
    "\n",
    "                        # Only keep images that have valid annotations\n",
    "                        if new_annotations:\n",
    "                            # Copy image with new name\n",
    "                            new_img_name = f\"img_{image_counter:06d}{img_file.suffix}\"\n",
    "                            new_img_path = self.output_dir / split / 'images' / new_img_name\n",
    "                            shutil.copy2(img_file, new_img_path)\n",
    "\n",
    "                            # Write new annotation file\n",
    "                            new_label_path = self.output_dir / split / 'labels' / f\"img_{image_counter:06d}.txt\"\n",
    "                            with open(new_label_path, 'w') as f:\n",
    "                                f.write('\\n'.join(new_annotations) + '\\n')\n",
    "\n",
    "                            image_counter += 1\n",
    "                            stats[split] += 1\n",
    "\n",
    "                            # Count classes\n",
    "                            for ann in new_annotations:\n",
    "                                class_id = int(ann.split()[0])\n",
    "                                class_name = self.final_classes[class_id]\n",
    "                                stats[f\"{split}_{class_name}\"] += 1\n",
    "\n",
    "        # Create new data.yaml\n",
    "        new_config = {\n",
    "            'path': str(self.output_dir.absolute()),\n",
    "            'train': 'train/images',\n",
    "            'val': 'valid/images',\n",
    "            'test': 'test/images',\n",
    "            'nc': len(self.final_classes),\n",
    "            'names': {i: name for i, name in enumerate(self.final_classes)}\n",
    "        }\n",
    "\n",
    "        # Write test split to val if test doesn't exist\n",
    "        if stats.get('test', 0) == 0 and stats.get('valid', 0) == 0:\n",
    "            print(\"No validation or test set found, creating validation split from train...\")\n",
    "            self.create_validation_split()\n",
    "            new_config['val'] = 'valid/images'\n",
    "\n",
    "        with open(self.output_dir / 'data.yaml', 'w') as f:\n",
    "            yaml.dump(new_config, f, default_flow_style=False)\n",
    "\n",
    "        # Print statistics\n",
    "        self.print_statistics(stats)\n",
    "\n",
    "        return str(self.output_dir)\n",
    "\n",
    "    def create_validation_split(self, val_ratio=0.2):\n",
    "        \"\"\"Create validation split from training data\"\"\"\n",
    "        import random\n",
    "\n",
    "        train_images = list((self.output_dir / 'train' / 'images').glob('*'))\n",
    "        random.shuffle(train_images)\n",
    "\n",
    "        val_count = int(len(train_images) * val_ratio)\n",
    "        val_images = train_images[:val_count]\n",
    "\n",
    "        for img_path in val_images:\n",
    "            # Move image\n",
    "            val_img_path = self.output_dir / 'valid' / 'images' / img_path.name\n",
    "            shutil.move(str(img_path), str(val_img_path))\n",
    "\n",
    "            # Move corresponding label\n",
    "            label_path = self.output_dir / 'train' / 'labels' / f\"{img_path.stem}.txt\"\n",
    "            if label_path.exists():\n",
    "                val_label_path = self.output_dir / 'valid' / 'labels' / f\"{img_path.stem}.txt\"\n",
    "                shutil.move(str(label_path), str(val_label_path))\n",
    "\n",
    "    def print_statistics(self, stats):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        total_images = sum(stats[split] for split in ['train', 'valid', 'test'] if split in stats)\n",
    "        print(f\"Total images: {total_images}\")\n",
    "\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            if split in stats:\n",
    "                print(f\"{split.capitalize()} images: {stats[split]}\")\n",
    "\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for class_name in self.final_classes:\n",
    "            total_instances = sum(stats.get(f\"{split}_{class_name}\", 0)\n",
    "                                  for split in ['train', 'valid', 'test'])\n",
    "            print(f\"  {class_name}: {total_instances} instances\")\n",
    "\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                split_count = stats.get(f\"{split}_{class_name}\", 0)\n",
    "                if split_count > 0:\n",
    "                    print(f\"    {split}: {split_count}\")\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def cleanup_temp_datasets(self, dataset_paths):\n",
    "        \"\"\"Remove temporary downloaded datasets\"\"\"\n",
    "        print(\"Cleaning up temporary files...\")\n",
    "        for dataset_path in dataset_paths:\n",
    "            if os.path.exists(dataset_path):\n",
    "                shutil.rmtree(dataset_path)\n",
    "        print(\"Cleanup completed!\")\n",
    "\n",
    "    def run(self, cleanup=True):\n",
    "        \"\"\"Main execution function\"\"\"\n",
    "        try:\n",
    "            # Download datasets\n",
    "            dataset_paths = self.download_datasets()\n",
    "\n",
    "            if not dataset_paths:\n",
    "                print(\"No datasets were downloaded successfully!\")\n",
    "                return None\n",
    "\n",
    "            # Combine datasets\n",
    "            combined_path = self.combine_datasets(dataset_paths)\n",
    "\n",
    "            # Cleanup temporary files\n",
    "            if cleanup:\n",
    "                self.cleanup_temp_datasets(dataset_paths)\n",
    "\n",
    "            print(f\"\\nDataset combination completed!\")\n",
    "            print(f\"Combined dataset saved to: {combined_path}\")\n",
    "            print(f\"Use the data.yaml file for YOLO training\")\n",
    "\n",
    "            return combined_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dataset combination: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "# Initialize the combiner\n",
    "combiner = DatasetCombiner(\n",
    "    api_key=os.getenv('ROBOFLOW_API_KEY'),\n",
    "    output_dir=\"wildlife_dataset\"\n",
    ")\n",
    "\n",
    "# Run the combination process\n",
    "result_path = combiner.run(cleanup=True)\n",
    "\n",
    "if result_path:\n",
    "    print(f\"\\n🎉 Success! Your dataset is ready at: {result_path}\")\n",
    "    print(\"\\nTo train with YOLO:\")\n",
    "    print(f\"yolo train data={result_path}/data.yaml model=yolo11n.pt epochs=100 imgsz=640\")\n",
    "else:\n",
    "    print(\"❌ Dataset combination failed!\")"
   ],
   "id": "a6a5c54d29021735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets...\n",
      "Downloading dataset 1/5: my-first-project-xub7r\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in temp_dataset_1 to yolov11::  64%|██████▎   | 350208/550910 [00:13<00:07, 26812.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 339\u001B[39m\n\u001B[32m    333\u001B[39m combiner = DatasetCombiner(\n\u001B[32m    334\u001B[39m     api_key=os.getenv(\u001B[33m'\u001B[39m\u001B[33mROBOFLOW_API_KEY\u001B[39m\u001B[33m'\u001B[39m),\n\u001B[32m    335\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33mwildlife_dataset\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    336\u001B[39m )\n\u001B[32m    338\u001B[39m \u001B[38;5;66;03m# Run the combination process\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m339\u001B[39m result_path = \u001B[43mcombiner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcleanup\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result_path:\n\u001B[32m    342\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m🎉 Success! Your dataset is ready at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 308\u001B[39m, in \u001B[36mDatasetCombiner.run\u001B[39m\u001B[34m(self, cleanup)\u001B[39m\n\u001B[32m    305\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Main execution function\"\"\"\u001B[39;00m\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    307\u001B[39m     \u001B[38;5;66;03m# Download datasets\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m308\u001B[39m     dataset_paths = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdownload_datasets\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    310\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dataset_paths:\n\u001B[32m    311\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mNo datasets were downloaded successfully!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 71\u001B[39m, in \u001B[36mDatasetCombiner.download_datasets\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     69\u001B[39m     project = \u001B[38;5;28mself\u001B[39m.rf.workspace(workspace).project(project_name)\n\u001B[32m     70\u001B[39m     version = project.version(version_num)\n\u001B[32m---> \u001B[39m\u001B[32m71\u001B[39m     dataset = \u001B[43mversion\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43myolov11\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemp_dataset_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     72\u001B[39m     datasets.append(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtemp_dataset_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     73\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/roboflow/core/version.py:232\u001B[39m, in \u001B[36mVersion.download\u001B[39m\u001B[34m(self, model_format, location, overwrite)\u001B[39m\n\u001B[32m    229\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m json.JSONDecodeError:\n\u001B[32m    230\u001B[39m             response.raise_for_status()\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__download_zip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlink\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;28mself\u001B[39m.__extract_zip(location, model_format)\n\u001B[32m    234\u001B[39m \u001B[38;5;28mself\u001B[39m.__reformat_yaml(location, model_format)  \u001B[38;5;66;03m# TODO: is roboflow-python a place to be munging yaml files?\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/roboflow/core/version.py:556\u001B[39m, in \u001B[36mVersion.__download_zip\u001B[39m\u001B[34m(self, link, location, format)\u001B[39m\n\u001B[32m    554\u001B[39m total_length = \u001B[38;5;28mint\u001B[39m(response.headers.get(\u001B[33m\"\u001B[39m\u001B[33mcontent-length\u001B[39m\u001B[33m\"\u001B[39m))  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    555\u001B[39m desc = \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m TQDM_DISABLE \u001B[38;5;28;01melse\u001B[39;00m \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDownloading Dataset Version Zip in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlocation\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mformat\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1024\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtotal_length\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1024\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/requests/models.py:820\u001B[39m, in \u001B[36mResponse.iter_content.<locals>.generate\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    818\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    819\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m820\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw.stream(chunk_size, decode_content=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    821\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    822\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/urllib3/response.py:1066\u001B[39m, in \u001B[36mHTTPResponse.stream\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1064\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m._fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1066\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1068\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[32m   1069\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/urllib3/response.py:955\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt, decode_content, cache_content)\u001B[39m\n\u001B[32m    952\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) >= amt:\n\u001B[32m    953\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._decoded_buffer.get(amt)\n\u001B[32m--> \u001B[39m\u001B[32m955\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    957\u001B[39m flush_decoder = amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt != \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[32m    959\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/urllib3/response.py:879\u001B[39m, in \u001B[36mHTTPResponse._raw_read\u001B[39m\u001B[34m(self, amt, read1)\u001B[39m\n\u001B[32m    876\u001B[39m fp_closed = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m._fp, \u001B[33m\"\u001B[39m\u001B[33mclosed\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    878\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._error_catcher():\n\u001B[32m--> \u001B[39m\u001B[32m879\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt != \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[32m    882\u001B[39m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    887\u001B[39m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[32m    888\u001B[39m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[32m    889\u001B[39m         \u001B[38;5;28mself\u001B[39m._fp.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpha/school/ipvc/ano3/s2/aoop/projects/wtba/.venv/lib/python3.12/site-packages/urllib3/response.py:862\u001B[39m, in \u001B[36mHTTPResponse._fp_read\u001B[39m\u001B[34m(self, amt, read1)\u001B[39m\n\u001B[32m    859\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read1()\n\u001B[32m    860\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    861\u001B[39m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m862\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/http/client.py:479\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    477\u001B[39m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[32m    478\u001B[39m     amt = \u001B[38;5;28mself\u001B[39m.length\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m s = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[32m    481\u001B[39m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[32m    482\u001B[39m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[32m    483\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_conn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/socket.py:707\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m707\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    708\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    709\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/ssl.py:1252\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1248\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1249\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1250\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1251\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1252\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1253\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1254\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/ssl.py:1104\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1102\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1103\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1104\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1105\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1106\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "aa831c206720e940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!yolo detect train data=/teamspace/studios/this_studio/wildlife_dataset/data.yaml model=yolo11n.pt epochs=100 imgsz=640",
   "id": "28383a1299cca57d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
